{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† FinTherapy-8B: Finnish Mental Health AI Demo\n",
    "\n",
    "Welcome to the official demo for **FinTherapy-8B**, a specialized Finnish language model finetuned for empathetic and professional mental health dialogue.\n",
    "\n",
    "### ‚ÑπÔ∏è About the Model\n",
    "- **Base Model:** LumiOpen/Llama-Poro-2-8B-Instruct\n",
    "- **Adapter:** kidahatsu/fintherapy-8b\n",
    "- **Focus:** Therapeutic empathy, active listening, and mental health support in Finnish.\n",
    "\n",
    "### ‚öôÔ∏è Setup Instructions (Kaggle)\n",
    "1. Open the **Notebook Settings** (sidebar).\n",
    "2. Set **Accelerator** to `GPU T4 x 2`.\n",
    "3. Set **Internet** to `On`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Setup & Installation\n",
    "# We install specific libraries optimized for 4-bit quantization to fit on free-tier GPUs.\n",
    "# Warnings about dependency conflicts (like pydantic/gradio) are expected in pre-built environments and can be safely ignored.\n",
    "\n",
    "print(\"‚è≥ Installing dependencies... (This takes about 1 minute)\")\n",
    "!pip install -U -q transformers accelerate bitsandbytes peft\n",
    "print(\"‚úÖ Installation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Memory Management\n",
    "# Essential step: Clear GPU memory to ensure we have the full 15GB VRAM available.\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Please enable 'GPU T4 x 2' in settings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model Configuration\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# Model IDs\n",
    "ADAPTER_MODEL = \"kidahatsu/fintherapy-8b\"\n",
    "BASE_MODEL = \"LumiOpen/Llama-Poro-2-8B-Instruct\"\n",
    "\n",
    "# 4-bit Quantization Config\n",
    "# This reduces model size from ~16GB to ~6GB, allowing it to run fast on a single T4 GPU.\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load Models\n",
    "print(\"‚è≥ Loading Tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"‚è≥ Loading Base Model (4-bit)... (This may take 2-3 minutes)\")\n",
    "# 'device_map=\"auto\"' automatically assigns layers to the GPU\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(\"‚è≥ Loading FinTherapy Adapter...\")\n",
    "# Crucial: 'subfolder=\"adapter\"' points to the correct location in the Hugging Face repo\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    ADAPTER_MODEL,\n",
    "    subfolder=\"adapter\" \n",
    ")\n",
    "\n",
    "print(\"‚úÖ System Ready! Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Inference Helper Function\n",
    "\n",
    "def therapist_chat(user_input):\n",
    "    \"\"\"\n",
    "    Generates a therapeutic response using the correct chat formatting.\n",
    "    \"\"\"\n",
    "    # Format prompt clearly for the model\n",
    "    prompt = f\"USER: {user_input}\\nASSISTANT:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,        # Allow long enough responses\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,            # Enable sampling for more natural text\n",
    "            temperature=0.7,           # Control creativity (0.7 is good for chat)\n",
    "            top_p=0.9,                 # Nucleus sampling\n",
    "            repetition_penalty=1.1     # Prevent repeating phrases\n",
    "        )\n",
    "    \n",
    "    # Decode and extract only the assistant's part\n",
    "    full_response = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    response_only = full_response.split(\"ASSISTANT:\")[-1].strip()\n",
    "    \n",
    "    return response_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Sample Prompts\n",
    "Below are several test cases demonstrating how the model handles different mental health scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CASE 1: Anxiety / Ahdistus\n",
    "prompt = \"Olen tuntenut oloni todella ahdistuneeksi viime aikoina, enk√§ pysty rentoutumaan edes kotona.\"\n",
    "print(f\"üë§ USER: {prompt}\\n\")\n",
    "print(f\"ü§ñ ASSISTANT:\\n{therapist_chat(prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CASE 2: Sleep Problems / Univaikeudet\n",
    "prompt = \"Her√§√§n jatkuvasti aamuy√∂ll√§ nelj√§lt√§ enk√§ saa en√§√§ unta. Se alkaa uuvuttaa minua.\"\n",
    "print(f\"üë§ USER: {prompt}\\n\")\n",
    "print(f\"ü§ñ ASSISTANT:\\n{therapist_chat(prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CASE 3: Loneliness / Yksin√§isyys\n",
    "prompt = \"Minusta tuntuu, ett√§ kukaan ei oikeasti ymm√§rr√§ minua. Olen aivan yksin ajatusteni kanssa.\"\n",
    "print(f\"üë§ USER: {prompt}\\n\")\n",
    "print(f\"ü§ñ ASSISTANT:\\n{therapist_chat(prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CASE 4: Panic Attack / Paniikkikohtaus\n",
    "prompt = \"Syd√§meni hakkaa ja minusta tuntuu etten saa henke√§. Apua.\"\n",
    "print(f\"üë§ USER: {prompt}\\n\")\n",
    "print(f\"ü§ñ ASSISTANT:\\n{therapist_chat(prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ‚úçÔ∏è Interactive Mode\n",
    "# Uncomment the lines below to chat interactively!\n",
    "\n",
    "# while True:\n",
    "#     user_input = input(\"\\nSin√§: \")\n",
    "#     if user_input.lower() in [\"exit\", \"lopeta\"]:\n",
    "#         break\n",
    "#     response = therapist_chat(user_input)\n",
    "#     print(f\"Therapist: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
